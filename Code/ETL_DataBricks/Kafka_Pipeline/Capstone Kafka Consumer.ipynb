{"cells":[{"cell_type":"markdown","source":["# Consumer: 2021 Air Quality USA by County"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"611ed845-ce71-4515-b75b-78b249fc2f67"}}},{"cell_type":"code","source":["# Create a mount point\n# Creating a mount point to write to\n\nstorageAccount = \"gen10datafund2111\"\nstorageContainer = \"jadr-health-insights\"\nclientSecret = dbutils.secrets.get(scope = \"jadr_blob\", key = \"clientSecret\")\nclientid = dbutils.secrets.get(scope = \"jadr_blob\", key = \"clientid\")\nmount_point=\"/mnt/jadr\"\n\nconfigs = {\"fs.azure.account.auth.type\": \"OAuth\",\n   \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n   \"fs.azure.account.oauth2.client.id\": clientid,\n   \"fs.azure.account.oauth2.client.secret\": clientSecret, \n   \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/d46b54b2-a652-420b-aa5a-2ef7f8fc706e/oauth2/token\",\n   \"fs.azure.createRemoteFileSystemDuringInitialization\": \"true\"}\n\ntry:\n    dbutils.fs.unmount(mount_point)\nexcept:\n    pass\n\ndbutils.fs.mount(source = \"abfss://\"+storageContainer+\"@\"+storageAccount+\".dfs.core.windows.net/\", mount_point = mount_point, extra_configs = configs)\ndisplay(dbutils.fs.ls(\"/mnt/jadr\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02c4672d-b44a-457b-802f-66796e269b94"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/mnt/jadr has been unmounted.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/mnt/jadr has been unmounted.\n</div>"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/mnt/jadr/Data/","Data/",0,1643742636000],["dbfs:/mnt/jadr/ML-Models/","ML-Models/",0,1643906451000],["dbfs:/mnt/jadr/deleteme.txt","deleteme.txt",8,1643742578000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/jadr/Data/</td><td>Data/</td><td>0</td><td>1643742636000</td></tr><tr><td>dbfs:/mnt/jadr/ML-Models/</td><td>ML-Models/</td><td>0</td><td>1643906451000</td></tr><tr><td>dbfs:/mnt/jadr/deleteme.txt</td><td>deleteme.txt</td><td>8</td><td>1643742578000</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Error Callback Functions\ndef error_cb(err):\n    \"\"\" The error callback is used for generic client errors. These\n        errors are generally to be considered informational as the client will\n        automatically try to recover from all errors, and no extra action\n        is typically required by the application.\n        For this example however, we terminate the application if the client\n        is unable to connect to any broker (_ALL_BROKERS_DOWN) and on\n        authentication errors (_AUTHENTICATION). \"\"\"\n\n    print(\"Client error: {}\".format(err))\n    if err.code() == KafkaError._ALL_BROKERS_DOWN or \\\n       err.code() == KafkaError._AUTHENTICATION:\n        # Any exception raised from this callback will be re-raised from the\n        # triggering flush() or poll() call.\n        raise KafkaException(err)\n\n        \n        \n# Error Consumer Setup\nfrom confluent_kafka import Consumer\nfrom time import sleep\nimport uuid\nfrom confluent_kafka import Producer, Consumer, KafkaError, KafkaException\nimport json\n\n\n\n#KAFKA variables, Move to the OS variables or configuration\n# This will work in local Jupiter Notebook, but in a databrick, hiding config.py is tougher. \nconfluentClusterName = \"stage3talent\"\nconfluentBootstrapServers = \"pkc-ldvmy.centralus.azure.confluent.cloud:9092\"\nconfluentTopicName = \"jadr-AQI\"\nschemaRegistryUrl = \"https://psrc-gq7pv.westus2.azure.confluent.cloud\"\nconfluentApiKey = dbutils.secrets.get(scope = \"jadr_blob\", key = \"confluentApiKey\")\nconfluentSecret = dbutils.secrets.get(scope = \"jadr_blob\", key = \"confluentSecret\")\nconfluentRegistryApiKey = dbutils.secrets.get(scope = \"jadr_blob\", key = \"confluentRegistryApiKey\")\nconfluentRegistrySecret = dbutils.secrets.get(scope = \"jadr_blob\", key = \"confluentRegistrySecret\")\n\n\n\n#Kakfa Class Setup.\nc = Consumer({\n    'bootstrap.servers': confluentBootstrapServers,\n    'sasl.mechanism': 'PLAIN',\n    'security.protocol': 'SASL_SSL',\n    'sasl.username': confluentApiKey,\n    'sasl.password': confluentSecret,\n    'group.id': str(uuid.uuid1()),  # this will create a new consumer group on each invocation.\n    'auto.offset.reset': 'earliest',\n    'enable.auto.commit': True,\n    'error_cb': error_cb,\n})\n\nc.subscribe([confluentTopicName])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21032b6b-16c0-449b-a886-df31228cbc97"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# CONSUME\n\naString = {}\n\nkafkaListDictionaries = []\n\nwhile(True):\n    try:\n        msg = c.poll(timeout=1.0)\n        if msg is None:\n            break\n        elif msg.error():\n            print(\"Consumer error: {}\".format(msg.error()))\n            break\n        else:\n            aString=json.loads('{}'.format(msg.value().decode('utf-8')))\n            kafkaListDictionaries.append(aString)\n            c.commit()\n    except Exception as e:\n        print(e)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eaa42525-25e2-4d84-a163-25fcd175802b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Clean Kafka Data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ffb92a4-9498-43dc-8177-d2fde6018e80"}}},{"cell_type":"code","source":["# Convert list of dictionaries to pyspark dataframe. Remove duplicates.\ndf = spark.createDataFrame(kafkaListDictionaries)\ndf = df.distinct()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ec40ee3-99ed-4331-ba0f-171e0ec665b3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Transform Kafka Data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a0a4704-a41f-4948-9d58-738b93723489"}}},{"cell_type":"code","source":["# The dataset contains multiple measurements for each pollutant using different methods and metrics. We will narrow down the measurements used so we can get only one per site per year.\nparameters = ['PM10 Total 0-10um STP',\n              'Lead (TSP) LC',\n              'PM2.5 - Local Conditions',\n              'Sulfur dioxide',\n              'Ozone',\n              'Nitrogen dioxide (NO2)']\npreferredMethods = ['Hi-Vol - ICAP SPECTRA (ICP-MS); 0.45M HNO3 Boil30 min',\n                    'INSTRUMENTAL - GAS PHASE CHEMILUMINESCENCE',\n                   'INSTRUMENTAL - CHEMILUMINESCENCE',\n                   'INSTRUMENTAL - ULTRA VIOLET ABSORPTION',\n                   'INSTRUMENTAL - ULTRA VIOLET',\n                   'Andersen RAAS2.5-300 PM2.5 SEQ w/WINS - GRAVIMETRIC',\n                   'R & P Model 2025 PM-2.5 Sequential Air Sampler w/VSCC - Gravimetric',\n                   'Multiple Methods Used',\n                   'INSTRUMENTAL - Pulsed Fluorescent 43C-TLE/43i-TLE']\npreferredMetrics = ['Daily Maximum 1-hour average',\n                    'Daily maxima of observed hourly values (between 9:00 AM and 8:00 PM)',\n                    'Daily Mean',\n                    'Daily maximum 1-hour average',\n                   'Observed Values']\ndf = df.filter(df.parameter.isin(parameters))\ndf = df.filter(df.method.isin(preferredMethods))\ndf = df.filter(df.metric_used.isin(preferredMetrics))\n\n# Handle bringing in some SO2 types we don't want with 'Observed Values'\ndf = df.filter(~((df.parameter == 'Sulfur dioxide') & \n    (df.metric_used == 'Observed Values')))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8eba782-1484-4400-aaa7-517921650a79"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Convert rows of different measurements per county to columns. Should end up with one row per county per year with columns for every measurement\nfrom pyspark.sql import functions as F\nAQIDFAgg = df.groupBy( 'state', 'county','year','parameter').pivot('parameter').agg(F.mean('arithmetic_mean'),\n                              F.mean('first_max_value'),\n                              F.mean('ninety_ninth_percentile'), \n                              F.mean('standard_deviation'),\n                              F.mean('second_max_value'),\n                              F.first('method'),\n                              F.first('metric_used'),                            \n                              F.first('units_of_measure'))\n\nAQIDFAgg = AQIDFAgg.withColumnRenamed(\"PM2.5 - Local Conditions_avg(ninety_ninth_percentile)\",\"PM25 - Local Conditions_avg(ninety_ninth_percentile)\")\nAQIDFAgg = AQIDFAgg.withColumnRenamed(\"PM2.5 - Local Conditions_avg(arithmetic_mean)\",\"PM25 - Local Conditions_avg(arithmetic_mean)\")\nAQIDFAgg = AQIDFAgg.withColumnRenamed(\"PM2.5 - Local Conditions_avg(first_max_value)\",\"PM25 - Local Conditions_avg(first_max_value)\")\nAQIDFAgg = AQIDFAgg.withColumnRenamed(\"PM2.5 - Local Conditions_avg(standard_deviation)\",\"PM25 - Local Conditions_avg(standard_deviation)\")\nAQIDFAgg = AQIDFAgg.withColumnRenamed(\"PM2.5 - Local Conditions_avg(second_max_value)\",\"PM25 - Local Conditions_avg(second_max_value)\")\nAQIDFAgg = AQIDFAgg.withColumnRenamed(\"PM2.5 - Local Conditions_first(method)\",\"PM25 - Local Conditions_first(method)\")\nAQIDFAgg = AQIDFAgg.withColumnRenamed(\"PM2.5 - Local Conditions_first(metric_used)\",\"PM25 - Local Conditions_first(metric_used)\")\nAQIDFAgg = AQIDFAgg.withColumnRenamed(\"PM2.5 - Local Conditions_first(units_of_measure)\",\"PM25 - Local Conditions_first(units_of_measure)\")\n\nAQIDFAgg2 = AQIDFAgg.groupBy('state','county','year').agg(F.first('Lead (TSP) LC_avg(arithmetic_mean)', ignorenulls=True).alias('LeadMean'),\n                             F.first('Lead (TSP) LC_avg(first_max_value)', ignorenulls=True).alias('Lead1stMax'),\n                              F.first('Lead (TSP) LC_avg(ninety_ninth_percentile)', ignorenulls=True).alias('Lead99perc'), \n                              F.first('Lead (TSP) LC_avg(standard_deviation)', ignorenulls=True).alias('LeadStd'),\n                              F.first('Lead (TSP) LC_avg(second_max_value)', ignorenulls=True).alias('Lead2ndMax'),\n                              F.first('Lead (TSP) LC_first(method)', ignorenulls=True).alias('LeadMethod'),\n                              F.first('Lead (TSP) LC_first(metric_used)', ignorenulls=True).alias('LeadMetric'),                            \n                              F.first('Lead (TSP) LC_first(units_of_measure)', ignorenulls=True).alias('LeadUnits'),\n                              F.first('Nitrogen dioxide (NO2)_avg(arithmetic_mean)', ignorenulls=True).alias('NO2Mean'),\n                              F.first('Nitrogen dioxide (NO2)_avg(first_max_value)', ignorenulls=True).alias('NO21stMax'),\n                              F.first('Nitrogen dioxide (NO2)_avg(ninety_ninth_percentile)', ignorenulls=True).alias('NO299perc'),\n                              F.first('Nitrogen dioxide (NO2)_avg(standard_deviation)', ignorenulls=True).alias('NO2Std'),\n                              F.first('Nitrogen dioxide (NO2)_avg(second_max_value)', ignorenulls=True).alias('NO22ndMax'),\n                              F.first('Nitrogen dioxide (NO2)_first(method)', ignorenulls=True).alias('NO2Method'),\n                              F.first('Nitrogen dioxide (NO2)_first(metric_used)', ignorenulls=True).alias('NO2Metric'),\n                              F.first('Nitrogen dioxide (NO2)_first(units_of_measure)', ignorenulls=True).alias('NO2Units'),\n                              F.first('Ozone_avg(arithmetic_mean)', ignorenulls=True).alias('OzoneMean'),\n                              F.first('Ozone_avg(first_max_value)', ignorenulls=True).alias('Ozone1stMax'),\n                              F.first('Ozone_avg(ninety_ninth_percentile)', ignorenulls=True).alias('Ozone99perc'),\n                              F.first('Ozone_avg(standard_deviation)', ignorenulls=True).alias('OzoneStd'),\n                              F.first('Ozone_avg(second_max_value)', ignorenulls=True).alias('Ozone2ndMax'),\n                              F.first('Ozone_first(method)', ignorenulls=True).alias('OzoneMethod'),\n                              F.first('Ozone_first(metric_used)', ignorenulls=True).alias('OzoneMetric'),\n                              F.first('Ozone_first(units_of_measure)', ignorenulls=True).alias('OzoneUnits'),\n                              F.first('PM10 Total 0-10um STP_avg(arithmetic_mean)', ignorenulls=True).alias('PM10Mean'),\n                              F.first('PM10 Total 0-10um STP_avg(first_max_value)', ignorenulls=True).alias('PM101stMax'),\n                              F.first('PM10 Total 0-10um STP_avg(ninety_ninth_percentile)', ignorenulls=True).alias('PM1099perc'),\n                              F.first('PM10 Total 0-10um STP_avg(standard_deviation)', ignorenulls=True).alias('PM10Std'),\n                              F.first('PM10 Total 0-10um STP_avg(second_max_value)', ignorenulls=True).alias('PM102ndMax'),\n                              F.first('PM10 Total 0-10um STP_first(method)', ignorenulls=True).alias('PM10Method'),\n                              F.first('PM10 Total 0-10um STP_first(metric_used)', ignorenulls=True).alias('PM10Metric'),\n                              F.first('PM10 Total 0-10um STP_first(units_of_measure)', ignorenulls=True).alias('PM10Units'),\n                              F.first('PM25 - Local Conditions_avg(arithmetic_mean)', ignorenulls=True).alias('PM25Mean'),\n                              F.first('PM25 - Local Conditions_avg(first_max_value)', ignorenulls=True).alias('PM251stMax'),\n                              F.first('PM25 - Local Conditions_avg(ninety_ninth_percentile)', ignorenulls=True).alias('PM2599perc'),\n                              F.first('PM25 - Local Conditions_avg(standard_deviation)', ignorenulls=True).alias('PM25Std'),\n                              F.first('PM25 - Local Conditions_avg(second_max_value)', ignorenulls=True).alias('PM252ndMax'),\n                              F.first('PM25 - Local Conditions_first(method)', ignorenulls=True).alias('PM25Method'),\n                              F.first('PM25 - Local Conditions_first(metric_used)', ignorenulls=True).alias('PM25Metric'),\n                              F.first('PM25 - Local Conditions_first(units_of_measure)', ignorenulls=True).alias('PM25Units'),\n                              F.first('Sulfur dioxide_avg(arithmetic_mean)', ignorenulls=True).alias('SO2Mean'),                                     \n                              F.first('Sulfur dioxide_avg(first_max_value)', ignorenulls=True).alias('SO21stMax'),\n                              F.first('Sulfur dioxide_avg(ninety_ninth_percentile)', ignorenulls=True).alias('SO299perc'),\n                              F.first('Sulfur dioxide_avg(standard_deviation)', ignorenulls=True).alias('SO2Std'),\n                              F.first('Sulfur dioxide_avg(second_max_value)', ignorenulls=True).alias('SO22ndMax'),\n                              F.first('Sulfur dioxide_first(method)', ignorenulls=True).alias('SO2Method'),\n                              F.first('Sulfur dioxide_first(metric_used)', ignorenulls=True).alias('SO2Metric'),\n                              F.first('Sulfur dioxide_first(units_of_measure)', ignorenulls=True).alias('SO2Units'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0dafce5b-345a-4dd9-8f3e-861323951b1d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Load Kafka Data (to DataLake)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c96d6a69-1888-422d-8dbf-6c923e7ebc92"}}},{"cell_type":"code","source":["# Write to the DataLake\nAQIDFAgg2.write.mode(\"append\").json('/mnt/jadr/Data/aqi_stream')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fca4eac8-0f79-489e-86f5-15a8dc4d7c2e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Capstone Kafka Consumer","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":522247351783707}},"nbformat":4,"nbformat_minor":0}
