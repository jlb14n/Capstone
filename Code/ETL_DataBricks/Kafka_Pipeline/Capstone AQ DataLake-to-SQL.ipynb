{"cells":[{"cell_type":"markdown","source":["# DataLake to SQL: 2021 Air Quality USA by County"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0eae7cac-4b46-434d-b192-d517539d4298"}}},{"cell_type":"code","source":["# imports\nfrom pyspark.sql.functions import isnan"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be34ab7a-1ddd-40de-bdb1-152f4bde3e48"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Create a mount point\n# Creating a mount point to write to\n\nstorageAccount = \"gen10datafund2111\"\nstorageContainer = \"jadr-health-insights\"\nclientSecret = dbutils.secrets.get(scope = \"jadr_blob\", key = \"clientSecret\")\nclientid = dbutils.secrets.get(scope = \"jadr_blob\", key = \"clientid\")\nmount_point=\"/mnt/jadr\"\n\nconfigs = {\"fs.azure.account.auth.type\": \"OAuth\",\n   \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n   \"fs.azure.account.oauth2.client.id\": clientid,\n   \"fs.azure.account.oauth2.client.secret\": clientSecret, \n   \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/d46b54b2-a652-420b-aa5a-2ef7f8fc706e/oauth2/token\",\n   \"fs.azure.createRemoteFileSystemDuringInitialization\": \"true\"}\n\ntry:\n    dbutils.fs.unmount(mount_point)\nexcept:\n    pass\n\ndbutils.fs.mount(source = \"abfss://\"+storageContainer+\"@\"+storageAccount+\".dfs.core.windows.net/\", mount_point = mount_point, extra_configs = configs)\ndisplay(dbutils.fs.ls(\"/mnt/jadr\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f1e6bac2-f3eb-415b-ba85-c2043b9b456a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/mnt/jadr has been unmounted.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/mnt/jadr has been unmounted.\n</div>"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/mnt/jadr/Data/","Data/",0,1643742636000],["dbfs:/mnt/jadr/ML-Models/","ML-Models/",0,1643906451000],["dbfs:/mnt/jadr/deleteme.txt","deleteme.txt",8,1643742578000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/jadr/Data/</td><td>Data/</td><td>0</td><td>1643742636000</td></tr><tr><td>dbfs:/mnt/jadr/ML-Models/</td><td>ML-Models/</td><td>0</td><td>1643906451000</td></tr><tr><td>dbfs:/mnt/jadr/deleteme.txt</td><td>deleteme.txt</td><td>8</td><td>1643742578000</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Read in the data and drop duplicates\ndf = spark.read.options(header=True).json('/mnt/jadr/Data/aqi_stream/*.json')\ndf = df.distinct()\nprint(f\"The number of rows are: {df.count()}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3669451a-4bde-487e-a840-6f21116135f5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">The number of rows are: 775\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The number of rows are: 775\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# SQL Database Connection\nserver = 'gen10-data-fundamentals-21-11-sql-server.database.windows.net'\ndatabase = 'jadr-SQL-Database'\nport = '1433'\nuser = dbutils.secrets.get(scope = \"jadr_blob\", key = \"SQLUser_dg\")\npassword = dbutils.secrets.get(scope = \"jadr_blob\", key = \"SQLPassword_dg\")\nurl = f\"jdbc:sqlserver://{server}:{port};databaseName={database};user={user};password={password};\" "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"017f3939-5420-4e6c-8816-55217907f60f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Convert to Foreign Keys"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1c6aa46-d71b-47fc-bffa-86e18eced2a8"}}},{"cell_type":"code","source":["### Read in the database County, State, Method, Metric, Unit tables\njdbcDF_state = spark.read.format(\"jdbc\") \\\n    .option(\"url\", url) \\\n    .option(\"dbtable\", \"State\") \\\n    .option(\"user\", user) \\\n    .option(\"password\", password) \\\n    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n    .load()\njdbcDF_county = spark.read.format(\"jdbc\") \\\n    .option(\"url\", url) \\\n    .option(\"dbtable\", \"County\") \\\n    .option(\"user\", user) \\\n    .option(\"password\", password) \\\n    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n    .load()\njdbcDF_method = spark.read.format(\"jdbc\") \\\n    .option(\"url\", url) \\\n    .option(\"dbtable\", \"Method\") \\\n    .option(\"user\", user) \\\n    .option(\"password\", password) \\\n    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n    .load()\njdbcDF_metric = spark.read.format(\"jdbc\") \\\n    .option(\"url\", url) \\\n    .option(\"dbtable\", \"Metric\") \\\n    .option(\"user\", user) \\\n    .option(\"password\", password) \\\n    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n    .load()\njdbcDF_unit = spark.read.format(\"jdbc\") \\\n    .option(\"url\", url) \\\n    .option(\"dbtable\", \"Unit\") \\\n    .option(\"user\", user) \\\n    .option(\"password\", password) \\\n    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n    .load()\n\n# Convert null to \"\" for future joining\njdbcDF_method = jdbcDF_method.fillna(\"\")\njdbcDF_metric = jdbcDF_metric.fillna(\"\")\njdbcDF_unit = jdbcDF_unit.fillna(\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5b3af64-f5eb-4269-a55c-cade02a56e3c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["### Change state to STATE_ID\n\n# Join on state name\ndf_join = df.join(jdbcDF_state, df.state == jdbcDF_state.STATE_NAME , \"leftouter\")\n\n# Write state to database if it doesn't yet exist\nif df_join.filter(df_join.STATE_ID.isNull()).count() > 0:\n    table = \"State\"\n    df_toload = df_join.filter(df_join.STATE_ID.isNull()).select(\"state\").withColumnRenamed(\"state\",\"STATE_NAME\").distinct()\n    \n    # Write the new state to the database\n    df_toload.write.format(\"jdbc\") \\\n        .option(\"url\", url) \\\n        .mode(\"append\") \\\n        .option(\"dbtable\", table) \\\n        .option(\"user\", user) \\\n        .option(\"password\", password) \\\n        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n        .save()\n\n    # Call the database again\n    jdbcDF_state = spark.read.format(\"jdbc\") \\\n        .option(\"url\", url) \\\n        .option(\"dbtable\", \"State\") \\\n        .option(\"user\", user) \\\n        .option(\"password\", password) \\\n        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n        .load()\n    \n    # Join on state name again\n    df_join = df.join(jdbcDF_state, df.state == jdbcDF_state.STATE_NAME , \"leftouter\") # Join the database again\n\n# Drop unnecessary columns\ndf = df_join.drop(\"state\",\"STATE_ABBR\",\"STATE_NAME\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd77284f-61dc-4f95-9fc0-8052b5d36339"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["### Change county and STATE_ID to COUNTY_ID\n\n# Join on county name and stateID. Drop the duplicate STATE_ID (not sure why it does that...)\ndf_join = df.join(jdbcDF_county, (df.county == jdbcDF_county.COUNTY_NAME) & (df.STATE_ID == jdbcDF_county.STATE_ID), \"leftouter\").drop(jdbcDF_county[\"STATE_ID\"])\n\n# Write county to database if it doesn't yet exist\nif df_join.filter(df_join.COUNTY_ID.isNull()).count() > 0:\n    table = \"County\"\n    df_toload = df_join.filter(df_join.COUNTY_ID.isNull()).select(\"STATE_ID\",\"county\").withColumnRenamed(\"county\",\"COUNTY_NAME\").distinct()\n    \n    # Write the new state to the database\n    df_toload.write.format(\"jdbc\") \\\n        .option(\"url\", url) \\\n        .mode(\"append\") \\\n        .option(\"dbtable\", table) \\\n        .option(\"user\", user) \\\n        .option(\"password\", password) \\\n        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n        .save()\n    \n    # Call the database again\n    jdbcDF_county = spark.read.format(\"jdbc\") \\\n        .option(\"url\", url) \\\n        .option(\"dbtable\", \"County\") \\\n        .option(\"user\", user) \\\n        .option(\"password\", password) \\\n        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n        .load()\n    \n    # Join on state name again\n    df_join = df.join(jdbcDF_county, (df.county == jdbcDF_county.COUNTY_NAME) & (df.STATE_ID == jdbcDF_county.STATE_ID), \"leftouter\").drop(jdbcDF_county[\"STATE_ID\"]) # Join the database again\n\n# Drop unnecessary columns\ndf = df_join.drop(\"county\",\"STATE_ID\",\"COUNTY_NAME\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d72f4aa5-5439-452a-821b-209ea5fd6dc1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Change methods to method_ID\npols=[\"Lead\",\"NO2\",\"Ozone\",\"PM10\",\"PM25\",\"SO2\"]\nfor pol in pols:\n    \n    # null =/= null with a join--replace with empty str literal\n    df = df.fillna(\"\",subset=f\"{pol}Method\")\n    \n    # Join on method name\n    df_join = df.join(jdbcDF_method, df[f\"{pol}Method\"] == jdbcDF_method.METHOD_NAME , \"leftouter\")\n    \n    # Write method to database if it doesn't yet exist\n    if df_join.filter(df_join.METHOD_ID.isNull()).count() > 0:\n        table = \"Method\"\n        df_toload = df_join.filter(df_join.METHOD_ID.isNull()).select(f\"{pol}Method\").withColumnRenamed(f\"{pol}Method\",\"METHOD_NAME\").distinct().replace(\"\",None)\n        display(df_toload)\n        # Write the new method(s) to the database\n        df_toload.write.format(\"jdbc\") \\\n            .option(\"url\", url) \\\n            .mode(\"append\") \\\n            .option(\"dbtable\", table) \\\n            .option(\"user\", user) \\\n            .option(\"password\", password) \\\n            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n            .save()\n\n        # Call the database again\n        jdbcDF_method = spark.read.format(\"jdbc\") \\\n            .option(\"url\", url) \\\n            .option(\"dbtable\", \"Method\") \\\n            .option(\"user\", user) \\\n            .option(\"password\", password) \\\n            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n            .load()\n        jdbcDF_method = jdbcDF_method.fillna(\"\")\n        \n        # Join on method name again\n        df_join = df.join(jdbcDF_method, df[f\"{pol}Method\"] == jdbcDF_method.METHOD_NAME , \"leftouter\") # Join the database again\n\n    # Drop unnecessary columns\n    df = df_join.drop(f\"{pol}Method\",\"METHOD_NAME\").withColumnRenamed(\"METHOD_ID\",f\"{pol.upper()}_METHOD_ID\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b2871c5-5e44-4286-98b2-7587dc4ff99e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#Change metric to metric_ID\npols=[\"Lead\",\"NO2\",\"Ozone\",\"PM10\",\"PM25\",\"SO2\"]\nfor pol in pols:\n    \n    # null =/= null with a join--replace with empty str literal\n    df = df.fillna(\"\",subset=f\"{pol}Metric\")\n    \n    # Join on metric name\n    df_join = df.join(jdbcDF_metric, df[f\"{pol}Metric\"] == jdbcDF_metric.METRIC_NAME , \"leftouter\")\n    \n    # Write metric to database if it doesn't yet exist\n    if df_join.filter(df_join.METRIC_ID.isNull()).count() > 0:\n        table = \"Metric\"\n        df_toload = df_join.filter(df_join.METRIC_ID.isNull()).select(f\"{pol}Metric\").withColumnRenamed(f\"{pol}Metric\",\"METRIC_NAME\").distinct().replace(\"\",None)\n        # Write the new metric(s) to the database\n        df_toload.write.format(\"jdbc\") \\\n            .option(\"url\", url) \\\n            .mode(\"append\") \\\n            .option(\"dbtable\", table) \\\n            .option(\"user\", user) \\\n            .option(\"password\", password) \\\n            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n            .save()\n\n        # Call the database again\n        jdbcDF_metric = spark.read.format(\"jdbc\") \\\n            .option(\"url\", url) \\\n            .option(\"dbtable\", \"Metric\") \\\n            .option(\"user\", user) \\\n            .option(\"password\", password) \\\n            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n            .load()\n        jdbcDF_metric = jdbcDF_metric.fillna(\"\")\n        \n        # Join on metric name again\n        df_join = df.join(jdbcDF_metric, df[f\"{pol}Metric\"] == jdbcDF_metric.METRIC_NAME , \"leftouter\") # Join the database again\n\n    # Drop unnecessary columns\n    df = df_join.drop(f\"{pol}Metric\",\"METRIC_NAME\").withColumnRenamed(\"METRIC_ID\",f\"{pol.upper()}_METRIC_ID\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eeef51ec-c41a-4c8b-ab5f-6100e6282fa2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Change units to units_ID\npols=[\"Lead\",\"NO2\",\"Ozone\",\"PM10\",\"PM25\",\"SO2\"]\nfor pol in pols:\n    \n    # null =/= null with a join--replace with empty str literal\n    df = df.fillna(\"\",subset=f\"{pol}Units\")\n    \n    # Join on unit name\n    df_join = df.join(jdbcDF_unit, df[f\"{pol}Units\"] == jdbcDF_unit.UNIT_NAME , \"leftouter\")\n\n    # Write unit to database if it doesn't yet exist\n    if df_join.filter(df_join.UNIT_ID.isNull()).count() > 0:\n        table = \"Unit\"\n        df_toload = df_join.filter(df_join.UNIT_ID.isNull()).select(f\"{pol}Units\").withColumnRenamed(f\"{pol}Units\",\"UNIT_NAME\").distinct().replace(\"\",None)\n        display(df_toload)\n        # Write the new unit(s) to the database\n        df_toload.write.format(\"jdbc\") \\\n            .option(\"url\", url) \\\n            .mode(\"append\") \\\n            .option(\"dbtable\", table) \\\n            .option(\"user\", user) \\\n            .option(\"password\", password) \\\n            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n            .save()\n\n        # Call the database again\n        jdbcDF_unit = spark.read.format(\"jdbc\") \\\n            .option(\"url\", url) \\\n            .option(\"dbtable\", \"Unit\") \\\n            .option(\"user\", user) \\\n            .option(\"password\", password) \\\n            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n            .load()\n        jdbcDF_unit = jdbcDF_unit.fillna(\"\")\n        \n        # Join on unit name again\n        df_join = df.join(jdbcDF_unit, df[f\"{pol}Units\"] == jdbcDF_unit.UNIT_NAME , \"leftouter\") # Join the database again\n\n    # Drop unnecessary columns\n    df = df_join.drop(f\"{pol}Units\",\"UNIT_NAME\").withColumnRenamed(\"UNIT_ID\",f\"{pol.upper()}_UNITS_ID\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1a0c393f-3de1-42ee-9832-8ab791f401c1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Change column names to match SQL Database"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4151755-792a-4230-92e8-439ce920b92f"}}},{"cell_type":"code","source":["# Copy schema from database and make DataFrame with that schema\n\n# Load in database\ndf_loaded = spark.read.format(\"jdbc\") \\\n    .option(\"url\", url) \\\n    .option(\"dbtable\", \"AirQualityDataCounty\") \\\n    .option(\"user\", user) \\\n    .option(\"password\", password) \\\n    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n    .load()\ndf_loaded = df_loaded.drop(\"AQ_ID\") # We don't need the IDs\n\n# Reorder columns to match order from database\ndf = df.select(\"COUNTY_ID\", \"year\",\n                             \"LeadMean\", \"Lead1stMax\", \"Lead99perc\", \"LeadStd\",\n                             \"Lead2ndMax\",\"LEAD_METHOD_ID\",\"LEAD_METRIC_ID\",\"LEAD_UNITS_ID\",\n                             \"NO2Mean\", \"NO21stMax\", \"NO299perc\", \"NO2Std\",\n                             \"NO22ndMax\", \"NO2_METHOD_ID\",\"NO2_METRIC_ID\",\"NO2_UNITS_ID\",\n                             \"OzoneMean\", \"Ozone1stMax\", \"Ozone99perc\", \"OzoneStd\",\n                             \"Ozone2ndMax\", \"OZONE_METHOD_ID\",\"OZONE_METRIC_ID\",\"OZONE_UNITS_ID\", \n                             \"PM10Mean\", \"PM101stMax\", \"PM1099perc\", \"PM10Std\",\n                             \"PM102ndMax\", \"PM10_METHOD_ID\",\"PM10_METRIC_ID\",\"PM10_UNITS_ID\",\n                             \"PM25Mean\", \"PM251stMax\", \"PM2599perc\", \"PM25Std\", \n                             \"PM252ndMax\", \"PM25_METHOD_ID\",\"PM25_METRIC_ID\",\"PM25_UNITS_ID\",\n                             \"SO2Mean\", \"SO21stMax\", \"SO299perc\", \"SO2Std\",\n                             \"SO22ndMax\", \"SO2_METHOD_ID\",\"SO2_METRIC_ID\",\"SO2_UNITS_ID\")\n\n#Replace schema\ndf = sqlContext.createDataFrame(df.rdd, df_loaded.schema)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53adeb1f-8d01-462e-963a-b1cf68b2a94f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Upload to SQL Database"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2aa59f5a-d32f-4bc1-b38f-5836bf89efc9"}}},{"cell_type":"code","source":["### Upload the Dataframe to the SQL database\ntable = \"AirQualityDataCounty\"\n\n# Subtract the two to only leave what is not in the database. Drop duplicates that occured (see next command for list of those duplicates)\ndf_toload = df.dropDuplicates(subset=[\"COUNTY_ID\",\"YEAR\"]).subtract(df_loaded)\n\n# Upload all the data that wasn't already in the database\ndf_toload.write.format(\"jdbc\").option(\"url\", f\"jdbc:sqlserver://{server}:1433;databaseName={database};\") \\\n    .mode(\"append\") \\\n    .option(\"dbtable\", table) \\\n    .option(\"user\", user) \\\n    .option(\"password\", password) \\\n    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n    .save()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b3c53fe-4fbf-421a-92f7-25dccba8caf5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# This shows which rows are being repeated due to duplicates from the consumer to datalake, possibly related to issues from producer to consumer. Worth investigating further if time allots.\ndf_copies = df.groupBy([\"COUNTY_ID\",\"YEAR\"]).count().filter(\"count > 1\")\ndisplay(df_copies)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6658bb3a-a674-4884-9328-1ef97195ec72"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1615,2021,2],[2343,2021,2],[54,2021,2],[3118,2021,2],[1983,2021,2],[473,2021,2],[2238,2021,2],[118,2021,2],[586,2021,2]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"COUNTY_ID","type":"\"integer\"","metadata":"{\"scale\":0}"},{"name":"YEAR","type":"\"integer\"","metadata":"{\"scale\":0}"},{"name":"count","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>COUNTY_ID</th><th>YEAR</th><th>count</th></tr></thead><tbody><tr><td>1615</td><td>2021</td><td>2</td></tr><tr><td>2343</td><td>2021</td><td>2</td></tr><tr><td>54</td><td>2021</td><td>2</td></tr><tr><td>3118</td><td>2021</td><td>2</td></tr><tr><td>1983</td><td>2021</td><td>2</td></tr><tr><td>473</td><td>2021</td><td>2</td></tr><tr><td>2238</td><td>2021</td><td>2</td></tr><tr><td>118</td><td>2021</td><td>2</td></tr><tr><td>586</td><td>2021</td><td>2</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Capstone AQ DataLake-to-SQL","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1536955998085752}},"nbformat":4,"nbformat_minor":0}
